{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Импорт-необходимых-библиотек\" data-toc-modified-id=\"Импорт-необходимых-библиотек-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Импорт необходимых библиотек</a></span></li><li><span><a href=\"#Открытие-файла\" data-toc-modified-id=\"Открытие-файла-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Открытие файла</a></span></li><li><span><a href=\"#Вывод-первых-десяти-строк-таблицы\" data-toc-modified-id=\"Вывод-первых-десяти-строк-таблицы-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Вывод первых десяти строк таблицы</a></span></li><li><span><a href=\"#Вывод-основной-информации-о-датафрейме\" data-toc-modified-id=\"Вывод-основной-информации-о-датафрейме-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Вывод основной информации о датафрейме</a></span></li><li><span><a href=\"#Преобразование-текста\" data-toc-modified-id=\"Преобразование-текста-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Преобразование текста</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Вывод</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Разбиение-данных-на-целевой-признак-и-признаки\" data-toc-modified-id=\"Разбиение-данных-на-целевой-признак-и-признаки-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Разбиение данных на целевой признак и признаки</a></span></li><li><span><a href=\"#Разбиение-данных-на-выборки\" data-toc-modified-id=\"Разбиение-данных-на-выборки-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Разбиение данных на выборки</a></span></li><li><span><a href=\"#Пайплайн\" data-toc-modified-id=\"Пайплайн-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Пайплайн</a></span></li><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Дерево-решений\" data-toc-modified-id=\"Дерево-решений-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Дерево решений</a></span></li><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Вывод</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "stoplist = set(nltk_stopwords.words('english')) \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Открытие файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv', engine='python')\n",
    "except:\n",
    "    data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод первых десяти строк таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод основной информации о датафрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразование текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим текст от лишних символов с помощью регулярных выражений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.replace(r'[^\\'a-zA-Z ]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизируем текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "data['text_lemmatized'] = data.text.apply(lemmatize_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_lemmatized'] = [item.lower() for item in data['text_lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные были преобразованы с помощью регулярных выражений и лемматизации. Необходимо их разбить на целевой признак и признаки — столбец `toxic` и столбец `text` соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на целевой признак и признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data['text_lemmatized']\n",
    "target = data['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение данных на выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем данные на обучающую и тестовую выборку в отношении 3:1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пайплайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_params(pipeline, params):\n",
    "    search = RandomizedSearchCV(pipeline, param_distributions=params, scoring='f1')\n",
    "    search.fit(features_train, target_train)\n",
    "    \n",
    "    return search.best_estimator_, search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(stop_words={'a', 'about', 'above', 'after',\n",
      "                                             'again', 'against', 'ain', 'all',\n",
      "                                             'am', 'an', 'and', 'any', 'are',\n",
      "                                             'aren', \"aren't\", 'as', 'at', 'be',\n",
      "                                             'because', 'been', 'before',\n",
      "                                             'being', 'below', 'between',\n",
      "                                             'both', 'but', 'by', 'can',\n",
      "                                             'couldn', \"couldn't\", ...})),\n",
      "                ('model',\n",
      "                 LogisticRegression(C=12.0, max_iter=1000,\n",
      "                                    random_state=12345))]) 0.7672258938077886\n"
     ]
    }
   ],
   "source": [
    "pipe_params = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words=stoplist)),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=12345))\n",
    "])\n",
    "\n",
    "params = {'model__C' : np.linspace(5, 12, 5)}\n",
    "\n",
    "best_reg, reg_f1 = search_params(pipe_params, params)\n",
    "print(best_reg, reg_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushi\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect',\n",
      "                 TfidfVectorizer(stop_words={'a', 'about', 'above', 'after',\n",
      "                                             'again', 'against', 'ain', 'all',\n",
      "                                             'am', 'an', 'and', 'any', 'are',\n",
      "                                             'aren', \"aren't\", 'as', 'at', 'be',\n",
      "                                             'because', 'been', 'before',\n",
      "                                             'being', 'below', 'between',\n",
      "                                             'both', 'but', 'by', 'can',\n",
      "                                             'couldn', \"couldn't\", ...})),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(max_depth=13, random_state=12345))]) 0.6073238834969654\n"
     ]
    }
   ],
   "source": [
    "pipe_params = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words=stoplist)),\n",
    "    ('model', DecisionTreeClassifier(random_state=12345))\n",
    "])\n",
    "\n",
    "params = {'model__max_depth' : range(5, 15, 2)}\n",
    "\n",
    "best_forest, forest_f1 = search_params(pipe_params, params)\n",
    "print(best_forest, forest_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью `Pipeline` и `RandomizedSearchCV` были подобраны такие параметры для моделей случайного леса и логистической регрессии, чтобы метрика F1 была наибольшей. Наилучшую метрику F1 на валидационной выборке показала модель логистической регрессии — около 0.77. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Проверим метрику F1 модели логистической регрессии на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778393351800554\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(best_reg.predict(features_test), target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика показала больше требуемых 0.75. \n",
    "\n",
    "Таким образом, данные были предобработаны с помощью регулярных выражений и лемматизации, после чего они были разбиты на тренировочную и тестовую выборки. Поиск гиперпараметров был совершен через `Pipeline` и `RandomizedSearchCV`. Наилучшей моделью, судя по метрике F1, оказалась модель логистической регрессии: на валидационной выборке она составила 0.76, а на валидационной — около 0.77."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1109px",
    "left": "28px",
    "top": "111.125px",
    "width": "724.389px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
